{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptim\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"human_vital_signs_dataset_2024.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Risk Category'].value_counts().plot.pie()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Risk Category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Risk Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Respiratory Rate'].isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "# Drop 'Patient ID' and 'Timestamp' as they are not predictive features\n",
    "df = df.drop(['Patient ID', 'Timestamp', 'Risk Category'], axis=1)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify existing derived features (no need to recreate if already correct)\n",
    "df['Pulse Pressure Check'] = df['Systolic Blood Pressure'] - df['Diastolic Blood Pressure']\n",
    "df['BMI Check'] = df['Weight (kg)'] / (df['Height (m)'] ** 2)\n",
    "df['MAP Check'] = (df['Systolic Blood Pressure'] + 2 * df['Diastolic Blood Pressure']) / 3\n",
    "\n",
    "# Compare with provided derived columns (optional, for validation)\n",
    "print(\"Derived feature validation:\")\n",
    "print(df[['Derived_Pulse_Pressure', 'Pulse Pressure Check', \n",
    "         'Derived_BMI', 'BMI Check', \n",
    "         'Derived_MAP', 'MAP Check']].head())\n",
    "\n",
    "# Drop check columns if they match the derived ones\n",
    "df = df.drop(['Pulse Pressure Check', 'BMI Check', 'MAP Check'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['Gender'], drop_first=True)\n",
    "df['Gender_Male'] = df['Gender_Male'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop('Respiratory Rate', axis=1).values  # Exclude Respiratory Rate\n",
    "y = df['Respiratory Rate'].values  # Target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)  # Reshape for regression\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# # Create DataLoaders\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, mlp_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                nn.LayerNorm(dim),\n",
    "                MultiHeadAttention(dim, heads, dropout),\n",
    "                nn.LayerNorm(dim),\n",
    "                FeedForward(dim, mlp_dim, dropout)\n",
    "            ]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for ln1, attn, ln2, ff in self.layers:\n",
    "            x = attn(ln1(x)) + x\n",
    "            x = ff(ln2(x)) + x\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = (dim // heads) ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.heads, C // self.heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class TabularViTUnetRegression(nn.Module):\n",
    "    def __init__(self, num_features, embed_dim=768, depth=12, heads=12, mlp_dim=3072, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vit_embeddings = nn.Linear(1, embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, num_features + 1, embed_dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.transformer = TransformerEncoder(embed_dim, depth, heads, mlp_dim, dropout)\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "        \n",
    "        # Regression head: outputs a single value\n",
    "        self.head = nn.Linear(embed_dim + 128, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, F = x.shape\n",
    "        x_vit = x.unsqueeze(-1)\n",
    "        x_vit = self.vit_embeddings(x_vit)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x_vit = torch.cat((cls_tokens, x_vit), dim=1)\n",
    "        x_vit = x_vit + self.pos_embedding\n",
    "        x_vit = self.dropout(x_vit)\n",
    "        x_vit = self.transformer(x_vit)\n",
    "        vit_out = x_vit[:, 0]\n",
    "        \n",
    "        mlp_out = self.mlp(x)\n",
    "        combined = torch.cat((vit_out, mlp_out), dim=1)\n",
    "        output = self.head(combined)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "# Then define DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=torch.Generator().manual_seed(seed)\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=torch.Generator().manual_seed(seed)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of batches in train_loader: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Initialize model\n",
    "num_features = X_train.shape[1]  # 13 features\n",
    "model = TabularViTUnetRegression(num_features)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()  # For regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)  # (batch_size, 1)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Evaluate on training set\n",
    "model.eval()\n",
    "train_loss = 0.0\n",
    "train_preds = []\n",
    "train_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        train_loss += loss.item()\n",
    "        train_preds.extend(outputs.cpu().numpy().flatten())\n",
    "        train_targets.extend(targets.cpu().numpy().flatten())\n",
    "\n",
    "# Compute training metrics\n",
    "train_loss = train_loss / len(train_loader)\n",
    "train_mse = mean_squared_error(train_targets, train_preds)\n",
    "train_mae = mean_absolute_error(train_targets, train_preds)\n",
    "train_r2 = r2_score(train_targets, train_preds)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss = 0.0\n",
    "test_preds = []\n",
    "test_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss += loss.item()\n",
    "        test_preds.extend(outputs.cpu().numpy().flatten())\n",
    "        test_targets.extend(targets.cpu().numpy().flatten())\n",
    "\n",
    "# Compute test metrics\n",
    "test_loss = test_loss / len(test_loader)\n",
    "test_mse = mean_squared_error(test_targets, test_preds)\n",
    "test_mae = mean_absolute_error(test_targets, test_preds)\n",
    "test_r2 = r2_score(test_targets, test_preds)\n",
    "\n",
    "# Print metrics\n",
    "print(\"Final Model Performance:\")\n",
    "print(f\"Train MSE Loss: {train_loss:.4f}, Train MAE: {train_mae:.4f}, Train R²: {train_r2:.4f}\")\n",
    "print(f\"Test MSE Loss: {test_loss:.4f}, Test MAE: {test_mae:.4f}, Test R²: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LinearRegression  # For permutation importance\n",
    "\n",
    "# Ensure model is on the correct device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate on test set to collect predictions\n",
    "model.eval()\n",
    "test_preds = []\n",
    "test_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        test_preds.extend(outputs.cpu().numpy().flatten())\n",
    "        test_targets.extend(targets.cpu().numpy().flatten())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "test_preds = np.array(test_preds)\n",
    "test_targets = np.array(test_targets)\n",
    "\n",
    "# Compute metrics\n",
    "test_mse = mean_squared_error(test_targets, test_preds)\n",
    "test_mae = mean_absolute_error(test_targets, test_preds)\n",
    "test_r2 = r2_score(test_targets, test_preds)\n",
    "print(\"Test Metrics:\")\n",
    "print(f\"MSE: {test_mse:.4f}, MAE: {test_mae:.4f}, R²: {test_r2:.4f}\")\n",
    "\n",
    "# Assume df has original data (for feature plots)\n",
    "# Columns for features\n",
    "feature_columns = ['Heart Rate', 'Body Temperature', 'Oxygen Saturation', \n",
    "                   'Systolic Blood Pressure', 'Diastolic Blood Pressure', 'Age', \n",
    "                   'Weight (kg)', 'Height (m)', 'Derived_HRV', 'Derived_Pulse_Pressure', \n",
    "                   'Derived_BMI', 'Derived_MAP', 'Gender_Male']\n",
    "\n",
    "# 1. Predicted vs. Actual Scatter Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(test_targets, test_preds, alpha=0.5, color='blue')\n",
    "plt.plot([test_targets.min(), test_targets.max()], [test_targets.min(), test_targets.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Respiratory Rate')\n",
    "plt.ylabel('Predicted Respiratory Rate')\n",
    "plt.title('Predicted vs. Actual Respiratory Rate')\n",
    "plt.show()\n",
    "\n",
    "# 2. Error Distribution Histogram\n",
    "errors = test_preds - test_targets\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(errors, bins=30, kde=True, color='purple')\n",
    "plt.xlabel('Prediction Error (Predicted - Actual)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Prediction Errors')\n",
    "plt.axvline(0, color='red', linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "# 3. Feature vs. Respiratory Rate Scatter Plots (select key features)\n",
    "key_features = ['Heart Rate', 'Age', 'Derived_BMI', 'Oxygen Saturation']\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "for i, feature in enumerate(key_features):\n",
    "    sns.scatterplot(data=df, x=feature, y='Respiratory Rate', alpha=0.5, ax=axes[i])\n",
    "    axes[i].set_title(f'Respiratory Rate vs. {feature}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Residual Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(test_preds, errors, alpha=0.5, color='green')\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel('Predicted Respiratory Rate')\n",
    "plt.ylabel('Residual (Predicted - Actual)')\n",
    "plt.title('Residual Plot')\n",
    "plt.show()\n",
    "\n",
    "# 5. Feature Importance Plot\n",
    "# Use a simple model (e.g., LinearRegression) for permutation importance\n",
    "X_test_np = X_test  # Assuming X_test is available from train_test_split\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_test_np, y_test)\n",
    "perm_importance = permutation_importance(lr, X_test_np, y_test, n_repeats=10, random_state=42)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_columns,\n",
    "    'Importance': perm_importance.importances_mean\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance, color='teal')\n",
    "plt.title('Feature Importance for Predicting Respiratory Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='Gender_Male', y='Respiratory Rate', data=df)\n",
    "plt.title('Respiratory Rate by Gender')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5416632,
     "sourceId": 8992827,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
