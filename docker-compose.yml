version: '3'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    hostname: zookeeper
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "13181:2181"
    networks:
      - kafka-network
      
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    hostname: kafka
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "9093:9093"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:9093
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
    networks:
      - kafka-network
    healthcheck:
      test: ["CMD", "cub", "kafka-ready", "-b", "localhost:9092", "1", "30"]
      interval: 10s
      timeout: 5s
      retries: 20
      
  kafka-init:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka-init
    depends_on:
      - kafka
    entrypoint: ['/bin/sh', '-c']
    command: |
      "
      # Wait for Kafka to be ready
      echo 'Waiting for Kafka to be ready...'
      cub kafka-ready -b kafka:9092 1 120
      
      # Create topics
      echo 'Creating topics...'
      kafka-topics --create --if-not-exists --topic sensor_raw --bootstrap-server kafka:9092 --replication-factor 1 --partitions 4
      kafka-topics --create --if-not-exists --topic validated_data --bootstrap-server kafka:9092 --replication-factor 1 --partitions 4
      kafka-topics --create --if-not-exists --topic inference_request --bootstrap-server kafka:9092 --replication-factor 1 --partitions 4
      kafka-topics --create --if-not-exists --topic inference_result --bootstrap-server kafka:9092 --replication-factor 1 --partitions 4
      kafka-topics --create --if-not-exists --topic anomaly_alert --bootstrap-server kafka:9092 --replication-factor 1 --partitions 4
      kafka-topics --create --if-not-exists --topic storage_event --bootstrap-server kafka:9092 --replication-factor 1 --partitions 4
      
      echo 'Topics created successfully!'
      "
    networks:
      - kafka-network
      
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    depends_on:
      - kafka
      - kafka-init
    ports:
      - "18080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
    networks:
      - kafka-network
      
  simulator:
    build:
      context: ./simulator
      dockerfile: Dockerfile
    container_name: sensor-simulator
    depends_on:
      kafka:
        condition: service_healthy
      kafka-init:
        condition: service_started
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      CSV_FILE: /data/human_vital_signs_dataset_2024.csv
    volumes:
      - ./simulator/data:/data
    networks:
      - kafka-network

  validation-service:
    build:
      context: ./validation_service
      dockerfile: Dockerfile
    container_name: validation-service
    depends_on:
      kafka:
        condition: service_healthy
      kafka-init:
        condition: service_started
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      KAFKA_INPUT_TOPIC: sensor_raw
      KAFKA_OUTPUT_TOPIC: validated_data
    networks:
      - kafka-network

  batching-service:
    build:
      context: ./batching_service
      dockerfile: Dockerfile
    container_name: batching-service
    depends_on:
      kafka:
        condition: service_healthy
      kafka-init:
        condition: service_started
      validation-service:
        condition: service_started
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      KAFKA_INPUT_TOPIC: validated_data
      KAFKA_OUTPUT_TOPIC: inference_request
      BATCH_WINDOW_SECONDS: 3
    networks:
      - kafka-network

  ml-inference-service:
    build:
      context: ./ml_inference_service
      dockerfile: Dockerfile
    container_name: ml-inference-service
    depends_on:
      kafka:
        condition: service_healthy
      kafka-init:
        condition: service_started
      batching-service:
        condition: service_started
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      KAFKA_INPUT_TOPIC: inference_request
      KAFKA_OUTPUT_TOPIC: inference_result
      ML_MODEL_PATH: ${ML_MODEL_PATH:-}
    networks:
      - kafka-network

  anomaly-analyzer:
    build:
      context: ./anomaly_analyzer
      dockerfile: Dockerfile
    container_name: anomaly-analyzer
    depends_on:
      kafka:
        condition: service_healthy
      kafka-init:
        condition: service_started
      ml-inference-service:
        condition: service_started
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      KAFKA_INPUT_TOPIC: inference_result
      KAFKA_ALERT_TOPIC: anomaly_alert
      KAFKA_STORAGE_TOPIC: storage_event
      CRITICAL_THRESHOLD: 0.7
    networks:
      - kafka-network

  redis:
    image: redis:7-alpine
    container_name: redis
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes
    volumes:
      - redis-data:/data
    networks:
      - kafka-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5

  alert-service:
    build:
      context: ./alert_service
      dockerfile: Dockerfile
    container_name: alert-service
    depends_on:
      kafka:
        condition: service_healthy
      kafka-init:
        condition: service_started
      redis:
        condition: service_healthy
      anomaly-analyzer:
        condition: service_started
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      KAFKA_INPUT_TOPIC: anomaly_alert
      REDIS_HOST: redis
      REDIS_PORT: 6379
      DEDUPLICATION_TTL_SECONDS: 300
      SMTP_HOST: ${SMTP_HOST:-}
      SMTP_USER: ${SMTP_USER:-}
      SMTP_PASSWORD: ${SMTP_PASSWORD:-}
      EMAIL_FROM: ${EMAIL_FROM:-alerts@healthmonitoring.com}
      EMAIL_TO: ${EMAIL_TO:-admin@healthmonitoring.com}
    volumes:
      - alert-logs:/var/log/alerts
    networks:
      - kafka-network

  storage-writer-service:
    build:
      context: ./storage_writer_service
      dockerfile: Dockerfile
    container_name: storage-writer-service
    depends_on:
      kafka:
        condition: service_healthy
      kafka-init:
        condition: service_started
      anomaly-analyzer:
        condition: service_started
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      KAFKA_INPUT_TOPIC: storage_event
      S3_BUCKET: ${S3_BUCKET:-health-monitoring-storage}
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID:-}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY:-}
      UPLOAD_TIME: ${UPLOAD_TIME:-00:00}
      LOCAL_FILES_DIR: /tmp/storage_files
      DELETE_AFTER_UPLOAD: ${DELETE_AFTER_UPLOAD:-true}
      FILE_GENERATION_INTERVAL_MINUTES: ${FILE_GENERATION_INTERVAL_MINUTES:-3}
    volumes:
      - storage-buffer:/tmp/storage_buffer
      - storage-files:/tmp/storage_files
    networks:
      - kafka-network
      
volumes:
  redis-data:
  alert-logs:
  storage-buffer:
  storage-files:

networks:
  kafka-network:
    driver: bridge
